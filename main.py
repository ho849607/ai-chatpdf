import os
import streamlit as st
from io import BytesIO
from dotenv import load_dotenv
import pdfplumber
from pptx import Presentation  # PPTX íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
import openai
from pathlib import Path
import hashlib
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from PIL import Image
import pytesseract
import subprocess  # hwp ì²˜ë¦¬ìš©
import tempfile

# Tesseract ê²½ë¡œ ì„¤ì • (Windows í™˜ê²½ì—ì„œ Tesseract-OCRì„ ì„¤ì¹˜í•œ ê²½ë¡œë¡œ ë³€ê²½)
# ì˜ˆ: pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
# ì•„ë˜ëŠ” í•„ìš”ì‹œ ì£¼ì„ í•´ì œ í›„ ì‚¬ìš©í•˜ì„¸ìš”.
# pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# ì´ˆê¸° ì„¤ì •
nltk.download('punkt')
nltk.download('stopwords')

# í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì •ì˜
korean_stopwords = ['ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë”', 'ë¡œ', 'ë¥¼', 'ì—',
                    'ì˜', 'ì€', 'ëŠ”', 'ê°€', 'ì™€', 'ê³¼', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤',
                    'ìœ¼ë¡œ', 'ì—ì„œ', 'ê¹Œì§€', 'ë¶€í„°', 'ê¹Œì§€', 'ë§Œ', 'í•˜ë‹¤', 'ê·¸ë¦¬ê³ ',
                    'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜']

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
dotenv_path = Path('.env')
load_dotenv(dotenv_path=dotenv_path)

# API í‚¤ ì„¤ì •
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    openai_api_key = st.sidebar.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", type="password")
    if not openai_api_key:
        st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()

openai.api_key = openai_api_key

# ì œëª© ì„¤ì • (Study Helperë¡œ ë³€ê²½)
st.title("ğŸ“š Study Helper")
st.write("---")

# 'lang' ì´ˆê¸°í™”
if 'lang' not in st.session_state:
    st.session_state.lang = 'english'  # ê¸°ë³¸ ì–¸ì–´ë¥¼ ì˜ì–´ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

# ì €ì‘ê¶Œ ìœ ì˜ì‚¬í•­ ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
st.warning("ì €ì‘ë¬¼ì„ ë¶ˆë²• ë³µì œí•˜ì—¬ ê²Œì‹œí•˜ëŠ” ê²½ìš° ë‹¹ì‚¬ëŠ” ì±…ì„ì§€ì§€ ì•Šìœ¼ë©°, ì €ì‘ê¶Œë²•ì— ìœ ì˜í•˜ì—¬ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”.")

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜ ì •ì˜
def chat_interface():
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for chat in st.session_state.chat_history:
        if chat["role"] == "user":
            with st.chat_message("user"):
                st.write(chat["message"])
        else:
            with st.chat_message("assistant"):
                st.write(chat["message"])

    # ì±„íŒ… ì…ë ¥
    if st.session_state.lang == 'korean':
        st.write("## ChatGPTì™€ì˜ ì±„íŒ…")
        user_chat_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    else:
        st.write("## Chat with ChatGPT")
        user_chat_input = st.chat_input("Enter your message:")

    if user_chat_input:
        add_chat_message("user", user_chat_input)
        with st.chat_message("user"):
            st.write(user_chat_input)

        with st.spinner("GPTê°€ ì‘ë‹µ ì¤‘ì…ë‹ˆë‹¤..."):
            gpt_response = ask_gpt_question(user_chat_input, st.session_state.lang)
            add_chat_message("assistant", gpt_response)
            with st.chat_message("assistant"):
                st.write(gpt_response)

def add_chat_message(role, message):
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    st.session_state.chat_history.append({"role": role, "message": message})

# PDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def pdf_to_text(upload_file):
    try:
        with pdfplumber.open(BytesIO(upload_file.getvalue())) as pdf:
            pages = []
            for i, page in enumerate(pdf.pages):
                text = page.extract_text()
                if text:
                    pages.append(f"<PAGE{i+1}>\n{text}")
            return "\n".join(pages)
    except Exception as e:
        st.error(f"PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

# PPTXì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def pptx_to_text(upload_file):
    try:
        prs = Presentation(BytesIO(upload_file.getvalue()))
        text_runs = []
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text_runs.append(shape.text)
        return "\n".join(text_runs)
    except Exception as e:
        st.error(f"PPTXì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

# ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•œê¸€+ì˜ì–´ ì§€ì›)
def image_to_text(uploaded_image):
    try:
        image = Image.open(uploaded_image)
        text = pytesseract.image_to_string(image, lang='kor+eng')  # í•œêµ­ì–´ì™€ ì˜ì–´ ì§€ì›
        return text
    except Exception as e:
        st.error(f'ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}')
        return ""

# HWPì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def hwp_to_text(upload_file):
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.hwp') as tmp:
            tmp.write(upload_file.getvalue())
            tmp_path = tmp.name
        # hwp5txt íˆ´ í•„ìš”
        result = subprocess.run(["hwp5txt", tmp_path], capture_output=True, text=True)
        if result.returncode == 0:
            return result.stdout
        else:
            st.error("HWPì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. hwp5txt íˆ´ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return ""
    except Exception as e:
        st.error(f"HWP ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

# ì–¸ì–´ ê°ì§€ í•¨ìˆ˜
def detect_language(text):
    try:
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
        prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ISO 639-1 ì½”ë“œë¡œ ê°ì§€í•´ ì£¼ì„¸ìš” (ì˜ˆ: 'en'ì€ ì˜ì–´, 'ko'ëŠ” í•œêµ­ì–´):\n\n{text[:500]}"
        messages = [HumanMessage(content=prompt)]
        response = llm(messages)
        language_code = response.content.strip().lower().split()[0]
        return language_code
    except Exception as e:
        st.error(f"ì–¸ì–´ ê°ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return "unknown"

# ìš”ì•½ ìƒì„± í•¨ìˆ˜
def summarize_pdf(text, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ì„œë¡ , ë³¸ë¡ , ê²°ë¡ ìœ¼ë¡œ êµ¬ì„±ëœ ìì„¸í•œ ìš”ì•½ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”:\n\n{text}"
    else:
        prompt = f"Read the following text and write a detailed summary with introduction, main body, and conclusion:\n\n{text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content.strip()

def extract_key_summary_words_with_sources(text, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
    if language == 'korean':
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ 5~10ê°œë¥¼ ì¶”ì¶œí•˜ê³ , ê° í‚¤ì›Œë“œì˜ ì¶œì²˜ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”.

í‚¤ì›Œë“œ1 (ì¶œì²˜)
í‚¤ì›Œë“œ2 (ì¶œì²˜)
...

í…ìŠ¤íŠ¸:
{text}"""
    else:
        prompt = f"""Extract 5 to 10 important keywords from the text and indicate their sources:

Keyword1 (Source)
Keyword2 (Source)
...

Text:
{text}"""
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content.strip()

def extract_and_search_terms(summary_text, extracted_text, language='english'):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ìš”ì•½ì—ì„œ ì¤‘ìš”í•œ ìš©ì–´ 5~10ê°œë¥¼ ì¶”ì¶œí•˜ê³ , ê° ìš©ì–´ ì •ì˜ì™€ í…ìŠ¤íŠ¸ ë‚´ í˜ì´ì§€ ì •ë³´ë¥¼ ì œê³µ:\n\n{summary_text}"
    else:
        prompt = f"From the following summary, extract 5-10 important terms, provide detailed definitions and their page references:\n\n{summary_text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content.strip()

def generate_quiz(text, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ê´€ì‹ 5ë¬¸ì œ ìƒì„±(4ì§€ì„ ë‹¤), ì •ë‹µ í‘œì‹œ:\n\n{text}"
    else:
        prompt = f"Based on the following content, create 5 multiple-choice questions (4 options) and indicate the correct answer:\n\n{text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content

def generate_exam_questions(text, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”í•œ ì‹œí—˜ ë¬¸ì œ 5ê°œë¥¼ ì œì‹œ:\n\n{text}"
    else:
        prompt = f"Based on the following content, provide 5 important exam questions:\n\n{text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content

def generate_questions_for_user(text, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìê°€ ê¹Šì´ ìƒê°í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ 3ê°œ ì œì‹œ:\n\n{text}"
    else:
        prompt = f"Based on the following content, generate 3 thoughtful questions for deeper understanding:\n\n{text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    questions = [q.strip() for q in response.content.strip().split('\n') if q.strip()]
    return questions

def ask_gpt_question(question, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€: {question}"
    else:
        prompt = question
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content

# ê²°ê³¼ë¥¼ PPTXë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def create_ppt_from_text(text, filename="summary_output.pptx"):
    prs = Presentation()
    # ê¸°ë³¸ ë ˆì´ì•„ì›ƒ ì‚¬ìš© (ì œëª© + ë³¸ë¬¸)
    title_slide_layout = prs.slide_layouts[0]
    slide = prs.slides.add_slide(title_slide_layout)
    slide.shapes.title.text = "Summary"
    slide.placeholders[1].text = text

    # ì¶”ê°€ì ì¸ ìŠ¬ë¼ì´ë“œë¥¼ ë§Œë“¤ê³  ì‹¶ë‹¤ë©´ í•„ìš”ì— ë”°ë¼ í™•ì¥ ê°€ëŠ¥
    buf = BytesIO()
    prs.save(buf)
    buf.seek(0)
    return buf

if "processed" not in st.session_state:
    st.session_state.processed = False

# íŒŒì¼ ì—…ë¡œë“œ (HWP í¬í•¨)
uploaded_file = st.file_uploader("íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš” (PDF, PPTX, PNG, JPG, JPEG, HWP ì§€ì›)", type=['pdf', 'pptx', 'png', 'jpg', 'jpeg', 'hwp'])

chat_interface()

if uploaded_file is not None:
    filename = uploaded_file.name
    extension = os.path.splitext(filename)[1].lower()

    if extension == ".pdf":
        file_bytes = uploaded_file.getvalue()
        file_hash = hashlib.md5(file_bytes).hexdigest()

        if ("uploaded_file_hash" not in st.session_state or
                st.session_state.uploaded_file_hash != file_hash):
            st.session_state.uploaded_file_hash = file_hash
            # ì´ì „ ê²°ê³¼ ì´ˆê¸°í™”
            st.session_state.extracted_text = ""
            st.session_state.summary = ""
            st.session_state.keywords = ""
            st.session_state.term_info = ""
            st.session_state.quiz = ""
            st.session_state.exam_questions = ""
            st.session_state.gpt_questions = []
            st.session_state.processed = False

        if not st.session_state.processed:
            extracted_text = pdf_to_text(uploaded_file)
            if not extracted_text.strip():
                st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                summary = ""
                st.session_state.summary = summary
            else:
                st.success("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.")
                language_code = detect_language(extracted_text)
                if language_code == 'ko':
                    lang = 'korean'
                    language_name = 'í•œêµ­ì–´'
                elif language_code == 'en':
                    lang = 'english'
                    language_name = 'ì˜ì–´'
                else:
                    lang = 'english'
                    language_name = 'ì•Œ ìˆ˜ ì—†ìŒ (ì˜ì–´ ì§„í–‰)'

                st.write(f"### ê°ì§€ëœ ì–¸ì–´: {language_name}")
                st.session_state.lang = lang
                st.session_state.extracted_text = extracted_text

                with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
                    summary = summarize_pdf(extracted_text, lang)
                    st.session_state.summary = summary

                with st.spinner("í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ ì¤‘..."):
                    key_summary_words = extract_key_summary_words_with_sources(extracted_text, lang)
                    st.session_state.keywords = key_summary_words

                with st.spinner("ì¤‘ìš” ë‹¨ì–´ ì •ë³´ ì¶”ì¶œ ì¤‘..."):
                    term_info = extract_and_search_terms(summary, extracted_text, language=lang)
                    st.session_state.term_info = term_info

                with st.spinner("í€´ì¦ˆ ìƒì„± ì¤‘..."):
                    quiz = generate_quiz(extracted_text, lang)
                    st.session_state.quiz = quiz

                with st.spinner("ì‹œí—˜ ë¬¸ì œ ìƒì„± ì¤‘..."):
                    exam_questions = generate_exam_questions(extracted_text, lang)
                    st.session_state.exam_questions = exam_questions

                with st.spinner("ì‚¬ìš©ììš© ì§ˆë¬¸ ìƒì„± ì¤‘..."):
                    gpt_questions = generate_questions_for_user(extracted_text, lang)
                    st.session_state.gpt_questions = gpt_questions

                st.session_state.processed = True
        else:
            extracted_text = st.session_state.extracted_text
            summary = st.session_state.summary
            key_summary_words = st.session_state.keywords
            term_info = st.session_state.term_info
            quiz = st.session_state.quiz
            exam_questions = st.session_state.exam_questions

    elif extension == ".pptx":
        file_bytes = uploaded_file.getvalue()
        file_hash = hashlib.md5(file_bytes).hexdigest()

        if ("uploaded_file_hash" not in st.session_state or
                st.session_state.uploaded_file_hash != file_hash):
            st.session_state.uploaded_file_hash = file_hash
            st.session_state.extracted_text = ""
            st.session_state.summary = ""
            st.session_state.keywords = ""
            st.session_state.term_info = ""
            st.session_state.quiz = ""
            st.session_state.exam_questions = ""
            st.session_state.gpt_questions = []
            st.session_state.processed = False

        if not st.session_state.processed:
            extracted_text = pptx_to_text(uploaded_file)
            if not extracted_text.strip():
                st.error("PPTXì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                summary = ""
                st.session_state.summary = summary
            else:
                st.success("PPTXì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.")
                language_code = detect_language(extracted_text)
                if language_code == 'ko':
                    lang = 'korean'
                    language_name = 'í•œêµ­ì–´'
                elif language_code == 'en':
                    lang = 'english'
                    language_name = 'ì˜ì–´'
                else:
                    lang = 'english'
                    language_name = 'ì•Œ ìˆ˜ ì—†ìŒ (ì˜ì–´ ì§„í–‰)'

                st.write(f"### ê°ì§€ëœ ì–¸ì–´: {language_name}")
                st.session_state.lang = lang
                st.session_state.extracted_text = extracted_text

                with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
                    summary = summarize_pdf(extracted_text, lang)
                    st.session_state.summary = summary

                with st.spinner("í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ ì¤‘..."):
                    key_summary_words = extract_key_summary_words_with_sources(extracted_text, lang)
                    st.session_state.keywords = key_summary_words

                with st.spinner("ì¤‘ìš” ë‹¨ì–´ ì •ë³´ ì¶”ì¶œ ì¤‘..."):
                    term_info = extract_and_search_terms(summary, extracted_text, language=lang)
                    st.session_state.term_info = term_info

                with st.spinner("í€´ì¦ˆ ìƒì„± ì¤‘..."):
                    quiz = generate_quiz(extracted_text, lang)
                    st.session_state.quiz = quiz

                with st.spinner("ì‹œí—˜ ë¬¸ì œ ìƒì„± ì¤‘..."):
                    exam_questions = generate_exam_questions(extracted_text, lang)
                    st.session_state.exam_questions = exam_questions

                with st.spinner("ì‚¬ìš©ììš© ì§ˆë¬¸ ìƒì„± ì¤‘..."):
                    gpt_questions = generate_questions_for_user(extracted_text, lang)
                    st.session_state.gpt_questions = gpt_questions

                st.session_state.processed = True
            else:
                extracted_text = st.session_state.extracted_text
                summary = st.session_state.summary
                key_summary_words = st.session_state.keywords
                term_info = st.session_state.term_info
                quiz = st.session_state.quiz
                exam_questions = st.session_state.exam_questions

    elif extension in [".png", ".jpg", ".jpeg"]:
        file_bytes = uploaded_file.getvalue()
        file_hash = hashlib.md5(file_bytes).hexdigest()

        if ("uploaded_file_hash" not in st.session_state or
                st.session_state.uploaded_file_hash != file_hash):
            st.session_state.uploaded_file_hash = file_hash
            st.session_state.extracted_text = ""
            st.session_state.summary = ""
            st.session_state.keywords = ""
            st.session_state.term_info = ""
            st.session_state.quiz = ""
            st.session_state.exam_questions = ""
            st.session_state.gpt_questions = []
            st.session_state.processed = False

        if not st.session_state.processed:
            extracted_text = image_to_text(uploaded_file)
            if not extracted_text.strip():
                st.error("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                summary = ""
                st.session_state.summary = summary
            else:
                st.success("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.")
                language_code = detect_language(extracted_text)
                if language_code == 'ko':
                    lang = 'korean'
                    language_name = 'í•œêµ­ì–´'
                elif language_code == 'en':
                    lang = 'english'
                    language_name = 'ì˜ì–´'
                else:
                    lang = 'english'
                    language_name = 'ì•Œ ìˆ˜ ì—†ìŒ (ì˜ì–´ ì§„í–‰)'

                st.write(f"### ê°ì§€ëœ ì–¸ì–´: {language_name}")
                st.session_state.lang = lang
                st.session_state.extracted_text = extracted_text

                with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
                    summary = summarize_pdf(extracted_text, lang)
                    st.session_state.summary = summary

                with st.spinner("í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ ì¤‘..."):
                    key_summary_words = extract_key_summary_words_with_sources(extracted_text, lang)
                    st.session_state.keywords = key_summary_words

                with st.spinner("ì¤‘ìš” ë‹¨ì–´ ì •ë³´ ì¶”ì¶œ ì¤‘..."):
                    term_info = extract_and_search_terms(summary, extracted_text, language=lang)
                    st.session_state.term_info = term_info

                with st.spinner("í€´ì¦ˆ ìƒì„± ì¤‘..."):
                    quiz = generate_quiz(extracted_text, lang)
                    st.session_state.quiz = quiz

                with st.spinner("ì‹œí—˜ ë¬¸ì œ ìƒì„± ì¤‘..."):
                    exam_questions = generate_exam_questions(extracted_text, lang)
                    st.session_state.exam_questions = exam_questions

                with st.spinner("ì‚¬ìš©ììš© ì§ˆë¬¸ ìƒì„± ì¤‘..."):
                    gpt_questions = generate_questions_for_user(extracted_text, lang)
                    st.session_state.gpt_questions = gpt_questions

                st.session_state.processed = True
            else:
                extracted_text = st.session_state.extracted_text
                summary = st.session_state.summary
                key_summary_words = st.session_state.keywords
                term_info = st.session_state.term_info
                quiz = st.session_state.quiz
                exam_questions = st.session_state.exam_questions

    elif extension == ".hwp":
        file_bytes = uploaded_file.getvalue()
        file_hash = hashlib.md5(file_bytes).hexdigest()

        if ("uploaded_file_hash" not in st.session_state or
                st.session_state.uploaded_file_hash != file_hash):
            st.session_state.uploaded_file_hash = file_hash
            st.session_state.extracted_text = ""
            st.session_state.summary = ""
            st.session_state.keywords = ""
            st.session_state.term_info = ""
            st.session_state.quiz = ""
            st.session_state.exam_questions = ""
            st.session_state.gpt_questions = []
            st.session_state.processed = False

        if not st.session_state.processed:
            extracted_text = hwp_to_text(uploaded_file)
            if not extracted_text.strip():
                st.error("HWPì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                summary = ""
                st.session_state.summary = summary
            else:
                st.success("HWPì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.")
                language_code = detect_language(extracted_text)
                if language_code == 'ko':
                    lang = 'korean'
                    language_name = 'í•œêµ­ì–´'
                elif language_code == 'en':
                    lang = 'english'
                    language_name = 'ì˜ì–´'
                else:
                    lang = 'english'
                    language_name = 'ì•Œ ìˆ˜ ì—†ìŒ (ì˜ì–´ ì§„í–‰)'

                st.write(f"### ê°ì§€ëœ ì–¸ì–´: {language_name}")
                st.session_state.lang = lang
                st.session_state.extracted_text = extracted_text

                with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
                    summary = summarize_pdf(extracted_text, lang)
                    st.session_state.summary = summary

                with st.spinner("í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ ì¤‘..."):
                    key_summary_words = extract_key_summary_words_with_sources(extracted_text, lang)
                    st.session_state.keywords = key_summary_words

                with st.spinner("ì¤‘ìš” ë‹¨ì–´ ì •ë³´ ì¶”ì¶œ ì¤‘..."):
                    term_info = extract_and_search_terms(summary, extracted_text, language=lang)
                    st.session_state.term_info = term_info

                with st.spinner("í€´ì¦ˆ ìƒì„± ì¤‘..."):
                    quiz = generate_quiz(extracted_text, lang)
                    st.session_state.quiz = quiz

                with st.spinner("ì‹œí—˜ ë¬¸ì œ ìƒì„± ì¤‘..."):
                    exam_questions = generate_exam_questions(extracted_text, lang)
                    st.session_state.exam_questions = exam_questions

                with st.spinner("ì‚¬ìš©ììš© ì§ˆë¬¸ ìƒì„± ì¤‘..."):
                    gpt_questions = generate_questions_for_user(extracted_text, lang)
                    st.session_state.gpt_questions = gpt_questions

                st.session_state.processed = True
            else:
                extracted_text = st.session_state.extracted_text
                summary = st.session_state.summary
                key_summary_words = st.session_state.keywords
                term_info = st.session_state.term_info
                quiz = st.session_state.quiz
                exam_questions = st.session_state.exam_questions

    else:
        st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF, PPTX, PNG, JPG, JPEG, HWP íŒŒì¼ë§Œ ì˜¬ë ¤ì£¼ì„¸ìš”.")

    # ê²°ê³¼ í‘œì‹œ
    if st.session_state.get("processed", False):
        if 'summary' in st.session_state and st.session_state.summary.strip():
            st.write("## ìš”ì•½ ê²°ê³¼")
            st.write(st.session_state.summary)
        else:
            st.write("## ìš”ì•½ ê²°ê³¼ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if ('keywords' in st.session_state and
                st.session_state.keywords.strip()):
            st.write("## í•µì‹¬ ìš”ì•½ ë‹¨ì–´ ë° ì¶œì²˜")
            st.write(st.session_state.keywords)

        if ('term_info' in st.session_state and
                st.session_state.term_info.strip()):
            st.write("## ìš”ì•½ ë‚´ ì¤‘ìš”í•œ ë‹¨ì–´ ì •ë³´")
            st.write(st.session_state.term_info)

        if 'quiz' in st.session_state and st.session_state.quiz.strip():
            st.write("## ìƒì„±ëœ í€´ì¦ˆ")
            st.write(st.session_state.quiz)

        if ('exam_questions' in st.session_state and
                st.session_state.exam_questions.strip()):
            st.write("## ìƒì„±ëœ ì‹œí—˜ ë¬¸ì œ")
            st.write(st.session_state.exam_questions)

        # PPT ìƒì„± ê¸°ëŠ¥ ì¶”ê°€
        st.write("---")
        if st.button("ìš”ì•½ ë‚´ìš©ì„ PPTë¡œ ë‹¤ìš´ë¡œë“œ"):
            ppt_buffer = create_ppt_from_text(st.session_state.summary)
            st.download_button(
                label="PPT ë‹¤ìš´ë¡œë“œ",
                data=ppt_buffer,
                file_name="summary_output.pptx",
                mime="application/vnd.openxmlformats-officedocument.presentationml.presentation"
            )

# í‚¤ì›Œë“œ ê²€ìƒ‰ ê¸°ëŠ¥
if st.session_state.get("processed", False):
    st.write("---")
    if st.session_state.lang == 'korean':
        st.write("## ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰")
        search_query = st.text_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    else:
        st.write("## ğŸ” Keyword Search")
        search_query = st.text_input("Enter a keyword to search:")

    if search_query:
        search_results = []
        for line in st.session_state.extracted_text.split('\n'):
            if search_query.lower() in line.lower():
                search_results.append(line.strip())
        if search_results:
            if st.session_state.lang == 'korean':
                st.write("### ê²€ìƒ‰ ê²°ê³¼:")
            else:
                st.write("### Search Results:")
            for result in search_results:
                st.write(f"- {result}")
        else:
            if st.session_state.lang == 'korean':
                st.write("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.write("No results found.")

# GPTê°€ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸
if st.session_state.get("processed", False):
    st.write("---")
    if st.session_state.lang == 'korean':
        st.write("## GPTê°€ ë‹¹ì‹ ì—ê²Œ ì§ˆë¬¸í•©ë‹ˆë‹¤")
    else:
        st.write("## GPT has questions for you")

    if "gpt_questions" in st.session_state:
        for idx, question in enumerate(st.session_state.gpt_questions):
            user_answer = st.text_input(f"**{question}**", key=f"gpt_question_{idx}")
            if user_answer:
                with st.spinner("GPTê°€ ì‘ë‹µ ê²€í†  ì¤‘..."):
                    if st.session_state.lang == 'korean':
                        feedback_prompt = f"{question}\n\nì‚¬ìš©ì ë‹µë³€: {user_answer}\n\ní”¼ë“œë°±ì„ ì œê³µí•´ ì£¼ì„¸ìš”."
                    else:
                        feedback_prompt = f"{question}\n\nUser's answer: {user_answer}\n\nPlease provide feedback."
                    feedback = ask_gpt_question(feedback_prompt, st.session_state.lang)
                    if st.session_state.lang == 'korean':
                        st.write("### GPTì˜ í”¼ë“œë°±")
                    else:
                        st.write("### GPT's Feedback")
                    st.write(feedback)

st.write("---")
st.info("**ChatGPTëŠ” ì‹¤ìˆ˜ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.**")





