import os
import streamlit as st
from io import BytesIO
from dotenv import load_dotenv
import pdfplumber
from pptx import Presentation
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
from langchain.callbacks import StreamingStdOutCallbackHandler
import openai
from pathlib import Path
import hashlib
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from PIL import Image
import pytesseract
import subprocess
import tempfile

# docx2txt ì„¤ì¹˜ í™•ì¸ (ì„¤ì¹˜ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´ exceptë¡œ ì²˜ë¦¬)
try:
    import docx2txt
    DOCX_ENABLED = True
except ImportError:
    DOCX_ENABLED = False

# Tesseract ê²½ë¡œ (ì‹¤í–‰ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
pytesseract.pytesseract.tesseract_cmd = r"/usr/bin/tesseract"

nltk.download('punkt')
nltk.download('stopwords')

korean_stopwords = [
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë”', 'ë¡œ', 'ë¥¼', 'ì—',
    'ì˜', 'ì€', 'ëŠ”', 'ê°€', 'ì™€', 'ê³¼', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤',
    'ìœ¼ë¡œ', 'ì—ì„œ', 'ê¹Œì§€', 'ë¶€í„°', 'ê¹Œì§€', 'ë§Œ', 'í•˜ë‹¤', 'ê·¸ë¦¬ê³ ',
    'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜'
]

dotenv_path = Path('.env')
load_dotenv(dotenv_path=dotenv_path)

openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    openai_api_key = st.sidebar.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", type="password")
    if not openai_api_key:
        st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()

openai.api_key = openai_api_key

# ------------------------ ì œëª©ì„ studyhelperë¡œ ë³€ê²½ ------------------------
st.title("studyhelper")
st.write("---")

if 'lang' not in st.session_state:
    st.session_state.lang = 'english'

st.warning("ì €ì‘ë¬¼ì„ ë¶ˆë²• ë³µì œí•˜ì—¬ ê²Œì‹œí•˜ëŠ” ê²½ìš° ë‹¹ì‚¬ëŠ” ì±…ì„ì§€ì§€ ì•Šìœ¼ë©°, ì €ì‘ê¶Œë²•ì— ìœ ì˜í•˜ì—¬ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”.")

def add_chat_message(role, message):
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    st.session_state.chat_history.append({"role": role, "message": message})

def ask_gpt_question(question, language):
    llm = ChatOpenAI(
        model_name="gpt-4", 
        temperature=0, 
        streaming=True, 
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€: {question}"
    else:
        prompt = question
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content

def chat_interface():
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for chat in st.session_state.chat_history:
        if chat["role"] == "user":
            with st.chat_message("user"):
                st.write(chat["message"])
        else:
            with st.chat_message("assistant"):
                st.write(chat["message"])

    if st.session_state.lang == 'korean':
        st.write("## ChatGPTì™€ì˜ ì±„íŒ… (GPT-4)")
        user_chat_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    else:
        st.write("## Chat with ChatGPT (GPT-4)")
        user_chat_input = st.chat_input("Enter your message:")

    if user_chat_input:
        add_chat_message("user", user_chat_input)
        with st.chat_message("user"):
            st.write(user_chat_input)

        with st.spinner("GPTê°€ ì‘ë‹µ ì¤‘ì…ë‹ˆë‹¤..."):
            gpt_response = ask_gpt_question(user_chat_input, st.session_state.lang)
            add_chat_message("assistant", gpt_response)
            with st.chat_message("assistant"):
                st.write(gpt_response)

def pdf_to_text(upload_file):
    try:
        with pdfplumber.open(BytesIO(upload_file.getvalue())) as pdf:
            pages = []
            for i, page in enumerate(pdf.pages):
                text = page.extract_text()
                if text:
                    pages.append(f"<PAGE{i+1}>\n{text}")
            return "\n".join(pages)
    except Exception as e:
        st.error(f"PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

def pptx_to_text(upload_file):
    try:
        prs = Presentation(BytesIO(upload_file.getvalue()))
        text_runs = []
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text_runs.append(shape.text)
        return "\n".join(text_runs)
    except Exception as e:
        st.error(f"PPTXì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

def image_to_text(uploaded_image):
    try:
        image = Image.open(uploaded_image)
        if not os.path.exists(pytesseract.pytesseract.tesseract_cmd):
            st.error("Tesseractê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ ê²½ë¡œê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return ""
        text = pytesseract.image_to_string(image, lang='kor+eng')
        return text
    except Exception as e:
        st.error(f'ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}')
        return ""

def hwp_to_text(upload_file):
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.hwp') as tmp:
            tmp.write(upload_file.getvalue())
            tmp_path = tmp.name
        result = subprocess.run(["hwp5txt", tmp_path], capture_output=True, text=True)
        if result.returncode == 0:
            return result.stdout
        else:
            st.error("HWPì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. hwp5txt íˆ´ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return ""
    except FileNotFoundError:
        st.error("hwp5txt ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. hwp5txtê°€ ì œëŒ€ë¡œ ì„¤ì¹˜ë˜ì–´ PATHì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return ""
    except Exception as e:
        st.error(f"HWP ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

def docx_to_text(upload_file):
    """docx2txt ì‚¬ìš© (ì„¤ì¹˜ ì•ˆ ëœ ê²½ìš° None ë°˜í™˜)"""
    if not DOCX_ENABLED:
        st.warning("docx2txtê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ .docx íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return ""
    try:
        text = docx2txt.process(BytesIO(upload_file.getvalue()))
        return text if text else ""
    except Exception as e:
        st.error(f"DOCX íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

def doc_to_text(upload_file):
    """DOC íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (antiword í•„ìš”)"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.doc') as tmp:
            tmp.write(upload_file.getvalue())
            tmp_path = tmp.name
        result = subprocess.run(["antiword", tmp_path], capture_output=True, text=True)
        if result.returncode == 0:
            return result.stdout
        else:
            st.error("DOCì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. antiword íˆ´ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return ""
    except FileNotFoundError:
        st.error("antiword ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. antiwordê°€ ì œëŒ€ë¡œ ì„¤ì¹˜ë˜ì–´ PATHì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return ""
    except Exception as e:
        st.error(f"DOC ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

def detect_language(text):
    llm = ChatOpenAI(
        model_name="gpt-4",
        temperature=0,
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ISO 639-1 ì½”ë“œë¡œ ê°ì§€í•´ ì£¼ì„¸ìš” (ì˜ˆ: 'en'ì€ ì˜ì–´, 'ko'ëŠ” í•œêµ­ì–´):\n\n{text[:500]}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    language_code = response.content.strip().lower().split()[0]
    return language_code

def summarize_text(text, language):
    llm = ChatOpenAI(
        model_name="gpt-4",
        temperature=0,
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    if language == 'korean':
        prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ì„œë¡ , ë³¸ë¡ , ê²°ë¡ ìœ¼ë¡œ êµ¬ì„±ëœ ìì„¸í•œ ìš”ì•½ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”:\n\n{text}"
    else:
        prompt = f"Read the following text and write a detailed summary with introduction, main body, and conclusion:\n\n{text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content.strip()

def extract_key_summary_words_with_sources(text, language):
    llm = ChatOpenAI(
        model_name="gpt-4",
        temperature=0,
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    if language == 'korean':
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ 5~10ê°œë¥¼ ì¶”ì¶œí•˜ê³ , ê° í‚¤ì›Œë“œì˜ ì¶œì²˜ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”.

í‚¤ì›Œë“œ1 (ì¶œì²˜)
í‚¤ì›Œë“œ2 (ì¶œì²˜)
...

í…ìŠ¤íŠ¸:
{text}"""
    else:
        prompt = f"""Extract 5 to 10 important keywords from the text and indicate their sources:

Keyword1 (Source)
Keyword2 (Source)
...

Text:
{text}"""
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content.strip()

def extract_and_search_terms(summary_text, extracted_text, language='english'):
    llm = ChatOpenAI(
        model_name="gpt-4",
        temperature=0,
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ìš”ì•½ì—ì„œ ì¤‘ìš”í•œ ìš©ì–´ 5~10ê°œë¥¼ ì¶”ì¶œí•˜ê³ , ê° ìš©ì–´ ì •ì˜ì™€ í…ìŠ¤íŠ¸ ë‚´ í˜ì´ì§€ ì •ë³´ë¥¼ ì œê³µ:\n\n{summary_text}"
    else:
        prompt = f"From the following summary, extract 5-10 important terms, provide detailed definitions and their page references:\n\n{summary_text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content.strip()

def generate_questions_for_user(text, language):
    llm = ChatOpenAI(
        model_name="gpt-4",
        temperature=0,
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìê°€ ê¹Šì´ ìƒê°í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ 3ê°œ ì œì‹œ:\n\n{text}"
    else:
        prompt = f"Based on the following content, generate 3 thoughtful questions for deeper understanding:\n\n{text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    questions = [q.strip() for q in response.content.strip().split('\n') if q.strip()]
    return questions

def create_ppt_from_text(text, filename="summary_output.pptx"):
    prs = Presentation()
    title_slide_layout = prs.slide_layouts[0]
    slide = prs.slides.add_slide(title_slide_layout)
    slide.shapes.title.text = "Summary"
    slide.placeholders[1].text = text

    buf = BytesIO()
    prs.save(buf)
    buf.seek(0)
    return buf

if "processed" not in st.session_state:
    st.session_state.processed = False

uploaded_file = st.file_uploader(
    "íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF, PPTX, ì´ë¯¸ì§€, HWP, DOC, DOCX)",
    type=['pdf', 'pptx', 'png', 'jpg', 'jpeg', 'hwp', 'doc', 'docx']
)

chat_interface()

if uploaded_file is not None:
    filename = uploaded_file.name
    extension = os.path.splitext(filename)[1].lower()

    file_bytes = uploaded_file.getvalue()
    file_hash = hashlib.md5(file_bytes).hexdigest()

    if ("uploaded_file_hash" not in st.session_state or
        st.session_state.uploaded_file_hash != file_hash):
        st.session_state.uploaded_file_hash = file_hash
        st.session_state.extracted_text = ""
        st.session_state.summary = ""
        st.session_state.keywords = ""
        st.session_state.term_info = ""
        st.session_state.gpt_questions = []
        st.session_state.processed = False

    if not st.session_state.processed:
        if extension == ".pdf":
            extracted_text = pdf_to_text(uploaded_file)
        elif extension == ".pptx":
            extracted_text = pptx_to_text(uploaded_file)
        elif extension in [".png", ".jpg", ".jpeg"]:
            extracted_text = image_to_text(uploaded_file)
        elif extension == ".hwp":
            extracted_text = hwp_to_text(uploaded_file)
        elif extension == ".docx":
            extracted_text = docx_to_text(uploaded_file)  # docx2txtê°€ ì—†ìœ¼ë©´ "" ë°˜í™˜
        elif extension == ".doc":
            extracted_text = doc_to_text(uploaded_file)   # antiword í•„ìš”
        else:
            st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF, PPTX, PNG, JPG, JPEG, HWP, DOC, DOCXë§Œ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            extracted_text = ""

        if not extracted_text.strip():
            st.error("ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.session_state.summary = ""
        else:
            st.success("í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")

            language_code = detect_language(extracted_text)
            if language_code == 'ko':
                lang = 'korean'
                language_name = 'í•œêµ­ì–´'
            elif language_code == 'en':
                lang = 'english'
                language_name = 'ì˜ì–´'
            else:
                lang = 'english'
                language_name = 'ì•Œ ìˆ˜ ì—†ìŒ (ì˜ì–´ ì§„í–‰)'

            st.write(f"### ê°ì§€ëœ ì–¸ì–´: {language_name}")
            st.session_state.lang = lang
            st.session_state.extracted_text = extracted_text

            with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
                summary = summarize_text(extracted_text, lang)
                st.session_state.summary = summary

            with st.spinner("í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ ì¤‘..."):
                key_summary_words = extract_key_summary_words_with_sources(extracted_text, lang)
                st.session_state.keywords = key_summary_words

            with st.spinner("ì¤‘ìš” ë‹¨ì–´ ì •ë³´ ì¶”ì¶œ ì¤‘..."):
                term_info = extract_and_search_terms(summary, extracted_text, language=lang)
                st.session_state.term_info = term_info

            with st.spinner("GPTê°€ ì§ˆë¬¸ì„ ìƒì„± ì¤‘..."):
                gpt_questions = generate_questions_for_user(extracted_text, lang)
                st.session_state.gpt_questions = gpt_questions

            st.session_state.processed = True

    if st.session_state.get("processed", False):
        if 'summary' in st.session_state and st.session_state.summary.strip():
            st.write("## ìš”ì•½ ê²°ê³¼")
            st.write(st.session_state.summary)
        else:
            st.write("## ìš”ì•½ ê²°ê³¼ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if 'keywords' in st.session_state and st.session_state.keywords.strip():
            st.write("## í•µì‹¬ ìš”ì•½ ë‹¨ì–´ ë° ì¶œì²˜")
            st.write(st.session_state.keywords)

        if 'term_info' in st.session_state and st.session_state.term_info.strip():
            st.write("## ìš”ì•½ ë‚´ ì¤‘ìš”í•œ ë‹¨ì–´ ì •ë³´")
            st.write(st.session_state.term_info)

        st.write("---")
        if st.button("ìš”ì•½ ë‚´ìš©ì„ PPTë¡œ ë‹¤ìš´ë¡œë“œ"):
            ppt_buffer = create_ppt_from_text(st.session_state.summary)
            st.download_button(
                label="PPT ë‹¤ìš´ë¡œë“œ",
                data=ppt_buffer,
                file_name="summary_output.pptx",
                mime="application/vnd.openxmlformats-officedocument.presentationml.presentation"
            )

if st.session_state.get("processed", False):
    st.write("---")
    if st.session_state.lang == 'korean':
        st.write("## ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰")
        search_query = st.text_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    else:
        st.write("## ğŸ” Keyword Search")
        search_query = st.text_input("Enter a keyword to search:")

    if search_query:
        search_results = []
        for line in st.session_state.extracted_text.split('\n'):
            if search_query.lower() in line.lower():
                search_results.append(line.strip())
        if search_results:
            if st.session_state.lang == 'korean':
                st.write("### ê²€ìƒ‰ ê²°ê³¼:")
            else:
                st.write("### Search Results:")
            for result in search_results:
                st.write(f"- {result}")
        else:
            if st.session_state.lang == 'korean':
                st.write("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.write("No results found.")

if st.session_state.get("processed", False):
    st.write("---")
    if st.session_state.lang == 'korean':
        st.write("## GPTê°€ ë‹¹ì‹ ì—ê²Œ ì§ˆë¬¸í•©ë‹ˆë‹¤")
    else:
        st.write("## GPT has questions for you")

    if "gpt_questions" in st.session_state:
        for idx, question in enumerate(st.session_state.gpt_questions):
            user_answer = st.text_input(f"**{question}**", key=f"gpt_question_{idx}")
            if user_answer:
                with st.spinner("GPTê°€ ì‘ë‹µì„ ê²€í†  ì¤‘ì…ë‹ˆë‹¤..."):
                    if st.session_state.lang == 'korean':
                        feedback_prompt = f"{question}\n\nì‚¬ìš©ì ë‹µë³€: {user_answer}\n\ní”¼ë“œë°±ì„ ì œê³µí•´ ì£¼ì„¸ìš”."
                    else:
                        feedback_prompt = f"{question}\n\nUser's answer: {user_answer}\n\nPlease provide feedback on this."
                    feedback = ask_gpt_question(feedback_prompt, st.session_state.lang)
                    if st.session_state.lang == 'korean':
                        st.write("### GPTì˜ í”¼ë“œë°±")
                    else:
                        st.write("### GPT's Feedback")
                    st.write(feedback)

st.write("---")
st.info("**âš  ChatGPTëŠ” ë•Œë•Œë¡œ ë¶€ì •í™•í•˜ê±°ë‚˜ ì˜¤í•´ì˜ ì†Œì§€ê°€ ìˆëŠ” ë‹µë³€ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ì •ë³´ëŠ” ì¶”ê°€ë¡œ ê²€ì¦í•˜ì„¸ìš”.**")
