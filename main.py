# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os
import streamlit as st
from io import BytesIO
from dotenv import load_dotenv
import pdfplumber
from pptx import Presentation  # PPTX íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
import openai
from pathlib import Path
import hashlib
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from PIL import Image  # ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pytesseract  # Tesseract OCR ë¼ì´ë¸ŒëŸ¬ë¦¬

# ì´ˆê¸° ì„¤ì •
nltk.download('punkt')
nltk.download('stopwords')

# í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì •ì˜
korean_stopwords = ['ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë”', 'ë¡œ', 'ë¥¼', 'ì—',
                    'ì˜', 'ì€', 'ëŠ”', 'ê°€', 'ì™€', 'ê³¼', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤',
                    'ìœ¼ë¡œ', 'ì—ì„œ', 'ê¹Œì§€', 'ë¶€í„°', 'ê¹Œì§€', 'ë§Œ', 'í•˜ë‹¤', 'ê·¸ë¦¬ê³ ',
                    'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜']

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
dotenv_path = Path('.env')
load_dotenv(dotenv_path=dotenv_path)

# API í‚¤ ì„¤ì •
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    openai_api_key = st.sidebar.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", type="password")
    if not openai_api_key:
        st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()

openai.api_key = openai_api_key

# Tesseract ê²½ë¡œ ì„¤ì • (Windows ì‚¬ìš©ìë¼ë©´ í•„ìš”)
# pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# ì œëª© ì„¤ì •
st.title("ğŸ“š PDF, PPTX ë° ì´ë¯¸ì§€ í•™ìŠµ ë„ìš°ë¯¸")
st.write("---")

# 'lang' ì´ˆê¸°í™”
if 'lang' not in st.session_state:
    st.session_state.lang = 'english'  # ê¸°ë³¸ ì–¸ì–´ë¥¼ ì˜ì–´ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

# ì €ì‘ê¶Œ ìœ ì˜ì‚¬í•­ ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
st.warning("ì €ì‘ë¬¼ì„ ë¶ˆë²• ë³µì œí•˜ì—¬ ê²Œì‹œí•˜ëŠ” ê²½ìš° ë‹¹ì‚¬ëŠ” ì±…ì„ì§€ì§€ ì•Šìœ¼ë©°, ì €ì‘ê¶Œë²•ì— ìœ ì˜í•˜ì—¬ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”.")

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜ ì •ì˜
def chat_interface():
    # ì±„íŒ… ê¸°ë¡ì„ ë©”ì¸ ì˜ì—­ì— í‘œì‹œ
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # ì±„íŒ… ê¸°ë¡ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
    for chat in st.session_state.chat_history:
        if chat["role"] == "user":
            with st.chat_message("user"):
                st.write(chat["message"])
        else:
            with st.chat_message("assistant"):
                st.write(chat["message"])

    # ì±„íŒ… ì…ë ¥ í•„ë“œì™€ ì œëª©
    if st.session_state.lang == 'korean':
        st.write("## ChatGPTì™€ì˜ ì±„íŒ…")
        user_chat_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    else:
        st.write("## Chat with ChatGPT")
        user_chat_input = st.chat_input("Enter your message:")

    if user_chat_input:
        # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì™€ í™”ë©´ì— ì¶”ê°€
        add_chat_message("user", user_chat_input)
        with st.chat_message("user"):
            st.write(user_chat_input)

        # GPTì˜ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
        with st.spinner("GPTê°€ ì‘ë‹µ ì¤‘ì…ë‹ˆë‹¤..."):
            gpt_response = ask_gpt_question(user_chat_input, st.session_state.lang)
            add_chat_message("assistant", gpt_response)
            with st.chat_message("assistant"):
                st.write(gpt_response)

# ì±„íŒ… ë©”ì‹œì§€ ì¶”ê°€ í•¨ìˆ˜
def add_chat_message(role, message):
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    st.session_state.chat_history.append({"role": role, "message": message})

# PDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def pdf_to_text(upload_file):
    try:
        with pdfplumber.open(BytesIO(upload_file.getvalue())) as pdf:
            pages = []
            for i, page in enumerate(pdf.pages):
                text = page.extract_text()
                if text:
                    pages.append(f"<PAGE{i+1}>\n{text}")
            return "\n".join(pages)
    except Exception as e:
        st.error(f"PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

# PPTXì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def pptx_to_text(upload_file):
    try:
        prs = Presentation(BytesIO(upload_file.getvalue()))
        text_runs = []
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text_runs.append(shape.text)
        return "\n".join(text_runs)
    except Exception as e:
        st.error(f"PPTXì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

# ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def image_to_text(uploaded_image):
    try:
        image = Image.open(uploaded_image)
        text = pytesseract.image_to_string(image, lang='kor+eng')  # í•œêµ­ì–´ì™€ ì˜ì–´ ì§€ì›
        return text
    except Exception as e:
        st.error(f'ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}')
        return ""

# ì–¸ì–´ ê°ì§€ í•¨ìˆ˜ (OpenAI API í™œìš©)
def detect_language(text):
    try:
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
        prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ISO 639-1 ì½”ë“œë¡œ ê°ì§€í•´ ì£¼ì„¸ìš” (ì˜ˆ: 'en'ì€ ì˜ì–´, 'ko'ëŠ” í•œêµ­ì–´):\n\n{text[:500]}"
        messages = [HumanMessage(content=prompt)]
        response = llm(messages)
        language_code = response.content.strip().lower()
        language_code = language_code.split()[0]
        return language_code
    except Exception as e:
        st.error(f"ì–¸ì–´ ê°ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return "unknown"

# ìš”ì•½ ìƒì„± í•¨ìˆ˜ (ì„œë¡ , ë³¸ë¡ , ê²°ë¡  êµ¬ì¡°ë¡œ)
def summarize_pdf(text, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ì„œë¡ , ë³¸ë¡ , ê²°ë¡ ìœ¼ë¡œ êµ¬ì„±ëœ ìì„¸í•œ ìš”ì•½ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”:\n\n{text}"
    else:
        prompt = f"Read the following text and write a detailed summary structured with an introduction, main body, and conclusion:\n\n{text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content.strip()

# í•µì‹¬ ìš”ì•½ ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜ (ì¶œì²˜ í¬í•¨)
def extract_key_summary_words_with_sources(text, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
    if language == 'korean':
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ 5~10ê°œë¥¼ ì¶”ì¶œí•˜ê³ , ê° í‚¤ì›Œë“œì˜ ì¶œì²˜(í˜ì´ì§€ ë²ˆí˜¸ ë˜ëŠ” ìœ„ì¹˜)ë¥¼ í‘œì‹œí•´ ì£¼ì„¸ìš”. ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ ì£¼ì„¸ìš”:

í‚¤ì›Œë“œ1 (ì¶œì²˜)
í‚¤ì›Œë“œ2 (ì¶œì²˜)
...

í…ìŠ¤íŠ¸:
{text}
"""
    else:
        prompt = f"""Extract 5 to 10 important keywords from the following text and indicate their sources (page number or location). Provide the results in the following format:

Keyword1 (Source)
Keyword2 (Source)
...

Text:
{text}
"""
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    keywords_with_sources = response.content.strip()
    return keywords_with_sources

# ë‹¨ì–´ ì¶”ì¶œ ë° ê²€ìƒ‰ í•¨ìˆ˜
def extract_and_search_terms(summary_text, extracted_text, language='english'):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ìš”ì•½ì—ì„œ ì¤‘ìš”í•œ ìš©ì–´ 5~10ê°œë¥¼ ì¶”ì¶œí•˜ê³ , ê° ìš©ì–´ì— ëŒ€í•œ ìì„¸í•œ ì •ì˜ì™€ í•´ë‹¹ ìš©ì–´ê°€ í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì–¸ê¸‰ëœ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”:\n\n{summary_text}"
    else:
        prompt = f"Extract 5 to 10 important terms from the following summary and provide a detailed definition and the page numbers where each term is mentioned in the text:\n\n{summary_text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    term_info = response.content.strip()
    return term_info

# í€´ì¦ˆ ìƒì„± í•¨ìˆ˜
def generate_quiz(text, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ 5ê°œì˜ ê°ê´€ì‹ í€´ì¦ˆ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”. ê° ì§ˆë¬¸ì€ 4ê°œì˜ ì„ íƒì§€ë¥¼ í¬í•¨í•˜ê³  ì •ë‹µì„ í‘œì‹œí•´ ì£¼ì„¸ìš”:\n\n{text}"
    else:
        prompt = f"Based on the following content, create 5 multiple-choice quiz questions. Each question should have 4 options and indicate the correct answer:\n\n{text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content

# ì‹œí—˜ ë¬¸ì œ ìƒì„± í•¨ìˆ˜
def generate_exam_questions(text, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”í•œ ì‹œí—˜ ë¬¸ì œ 5ê°œë¥¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”:\n\n{text}"
    else:
        prompt = f"Based on the following content, create 5 important exam questions:\n\n{text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content

# GPTê°€ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def generate_questions_for_user(text, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìê°€ ë” ê¹Šì´ ìƒê°í•  ìˆ˜ ìˆë„ë¡ 3ê°œì˜ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”:\n\n{text}"
    else:
        prompt = f"Based on the following content, generate 3 thoughtful questions to ask the user for a deeper understanding:\n\n{text}"
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    questions = response.content.strip().split('\n')
    return questions

# ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ GPT ì‘ë‹µ í•¨ìˆ˜
def ask_gpt_question(question, language):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
    if language == 'korean':
        prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”: {question}"
    else:
        prompt = question
    messages = [HumanMessage(content=prompt)]
    response = llm(messages)
    return response.content

# íŒŒì¼ ì—…ë¡œë“œ ë° ë°ì´í„° ì²˜ë¦¬
if "processed" not in st.session_state:
    st.session_state.processed = False

uploaded_file = st.file_uploader("PDF, PPTX ë˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”", type=['pdf', 'pptx', 'png', 'jpg', 'jpeg'])

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì—…ë¡œë” ë°”ë¡œ ì•„ë˜ì— í‘œì‹œí•©ë‹ˆë‹¤.
chat_interface()

if uploaded_file is not None:
    # íŒŒì¼ í™•ì¥ìë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ íƒ€ì… ê²°ì •
    filename = uploaded_file.name
    extension = os.path.splitext(filename)[1].lower()

    if extension == ".pdf":
        # PDF ì²˜ë¦¬ ì½”ë“œ
        # íŒŒì¼ì˜ ê³ ìœ  í•´ì‹œ ìƒì„±
        file_bytes = uploaded_file.getvalue()
        file_hash = hashlib.md5(file_bytes).hexdigest()

        # íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if ("uploaded_file_hash" not in st.session_state or
                st.session_state.uploaded_file_hash != file_hash):
            st.session_state.uploaded_file_hash = file_hash

            # ì´ì „ì— ì €ì¥ëœ ê²°ê³¼ ì´ˆê¸°í™”
            st.session_state.extracted_text = ""
            st.session_state.summary = ""
            st.session_state.keywords = ""
            st.session_state.term_info = ""
            st.session_state.quiz = ""
            st.session_state.exam_questions = ""
            st.session_state.gpt_questions = []
            st.session_state.processed = False

        # ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ì¬ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
        if not st.session_state.processed:
            extracted_text = pdf_to_text(uploaded_file)

            if not extracted_text.strip():
                st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ PDFë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
                summary = ""
                st.session_state.summary = summary
            else:
                st.success("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
                language_code = detect_language(extracted_text)
                if language_code == 'ko':
                    lang = 'korean'
                    language_name = 'í•œêµ­ì–´'
                elif language_code == 'en':
                    lang = 'english'
                    language_name = 'ì˜ì–´'
                else:
                    lang = 'english'  # ê¸°ë³¸ê°’ì„ ì˜ì–´ë¡œ ì„¤ì •
                    language_name = 'ì•Œ ìˆ˜ ì—†ìŒ (ì˜ì–´ë¡œ ì§„í–‰í•©ë‹ˆë‹¤)'

                st.write(f"### ê°ì§€ëœ ì–¸ì–´: {language_name}")
                st.session_state.lang = lang
                st.session_state.extracted_text = extracted_text

                # ìš”ì•½ ìƒì„± ë° ì €ì¥
                with st.spinner("ìš”ì•½ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    summary = summarize_pdf(extracted_text, lang)
                    st.session_state.summary = summary

                # í•µì‹¬ ìš”ì•½ ë‹¨ì–´ ì¶”ì¶œ (ì¶œì²˜ í¬í•¨)
                with st.spinner("í•µì‹¬ ìš”ì•½ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    key_summary_words = extract_key_summary_words_with_sources(
                        extracted_text, lang)
                    st.session_state.keywords = key_summary_words

                # ì¤‘ìš” ë‹¨ì–´ ì •ë³´ ì¶”ì¶œ
                with st.spinner("ìš”ì•½ ë‚´ ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    term_info = extract_and_search_terms(summary,
                                                         extracted_text,
                                                         language=lang)
                    st.session_state.term_info = term_info

                # í€´ì¦ˆ ìƒì„±
                with st.spinner("í€´ì¦ˆë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    quiz = generate_quiz(extracted_text, lang)
                    st.session_state.quiz = quiz

                # ì‹œí—˜ ë¬¸ì œ ìƒì„±
                with st.spinner("ì‹œí—˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    exam_questions = generate_exam_questions(extracted_text,
                                                             lang)
                    st.session_state.exam_questions = exam_questions

                # GPTê°€ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸ ìƒì„±
                with st.spinner("GPTê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    gpt_questions = generate_questions_for_user(extracted_text,
                                                                lang)
                    st.session_state.gpt_questions = gpt_questions

                st.session_state.processed = True
        else:
            # ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ì„¸ì…˜ ìƒíƒœì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            extracted_text = st.session_state.extracted_text
            summary = st.session_state.summary
            key_summary_words = st.session_state.keywords
            term_info = st.session_state.term_info
            quiz = st.session_state.quiz
            exam_questions = st.session_state.exam_questions

        # ê²°ê³¼ í‘œì‹œ
        if 'summary' in st.session_state and st.session_state.summary.strip():
            st.write("## ìš”ì•½ ê²°ê³¼")
            st.write(st.session_state.summary)
        else:
            st.write("## ìš”ì•½ ê²°ê³¼ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if ('keywords' in st.session_state and
                st.session_state.keywords.strip()):
            st.write("## í•µì‹¬ ìš”ì•½ ë‹¨ì–´ ë° ì¶œì²˜")
            st.write(st.session_state.keywords)

        if ('term_info' in st.session_state and
                st.session_state.term_info.strip()):
            st.write("## ìš”ì•½ ë‚´ ì¤‘ìš”í•œ ë‹¨ì–´ ì •ë³´")
            st.write(st.session_state.term_info)

        if 'quiz' in st.session_state and st.session_state.quiz.strip():
            st.write("## ìƒì„±ëœ í€´ì¦ˆ")
            st.write(st.session_state.quiz)

        if ('exam_questions' in st.session_state and
                st.session_state.exam_questions.strip()):
            st.write("## ìƒì„±ëœ ì‹œí—˜ ë¬¸ì œ")
            st.write(st.session_state.exam_questions)

    elif extension == ".pptx":
        # PPTX ì²˜ë¦¬ ì½”ë“œ
        # íŒŒì¼ì˜ ê³ ìœ  í•´ì‹œ ìƒì„±
        file_bytes = uploaded_file.getvalue()
        file_hash = hashlib.md5(file_bytes).hexdigest()

        # íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if ("uploaded_file_hash" not in st.session_state or
                st.session_state.uploaded_file_hash != file_hash):
            st.session_state.uploaded_file_hash = file_hash

            # ì´ì „ì— ì €ì¥ëœ ê²°ê³¼ ì´ˆê¸°í™”
            st.session_state.extracted_text = ""
            st.session_state.summary = ""
            st.session_state.keywords = ""
            st.session_state.term_info = ""
            st.session_state.quiz = ""
            st.session_state.exam_questions = ""
            st.session_state.gpt_questions = []
            st.session_state.processed = False

        if not st.session_state.processed:
            extracted_text = pptx_to_text(uploaded_file)

            if not extracted_text.strip():
                st.error("PPTXì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ PPTX íŒŒì¼ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
                summary = ""
                st.session_state.summary = summary
            else:
                st.success("PPTXì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
                language_code = detect_language(extracted_text)
                if language_code == 'ko':
                    lang = 'korean'
                    language_name = 'í•œêµ­ì–´'
                elif language_code == 'en':
                    lang = 'english'
                    language_name = 'ì˜ì–´'
                else:
                    lang = 'english'  # ê¸°ë³¸ê°’ì„ ì˜ì–´ë¡œ ì„¤ì •
                    language_name = 'ì•Œ ìˆ˜ ì—†ìŒ (ì˜ì–´ë¡œ ì§„í–‰í•©ë‹ˆë‹¤)'

                st.write(f"### ê°ì§€ëœ ì–¸ì–´: {language_name}")
                st.session_state.lang = lang
                st.session_state.extracted_text = extracted_text

                # ìš”ì•½ ìƒì„± ë° ì €ì¥
                with st.spinner("ìš”ì•½ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    summary = summarize_pdf(extracted_text, lang)
                    st.session_state.summary = summary

                # í•µì‹¬ ìš”ì•½ ë‹¨ì–´ ì¶”ì¶œ (ì¶œì²˜ í¬í•¨)
                with st.spinner("í•µì‹¬ ìš”ì•½ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    key_summary_words = extract_key_summary_words_with_sources(
                        extracted_text, lang)
                    st.session_state.keywords = key_summary_words

                # ì¤‘ìš” ë‹¨ì–´ ì •ë³´ ì¶”ì¶œ
                with st.spinner("ìš”ì•½ ë‚´ ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    term_info = extract_and_search_terms(summary,
                                                         extracted_text,
                                                         language=lang)
                    st.session_state.term_info = term_info

                # í€´ì¦ˆ ìƒì„±
                with st.spinner("í€´ì¦ˆë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    quiz = generate_quiz(extracted_text, lang)
                    st.session_state.quiz = quiz

                # ì‹œí—˜ ë¬¸ì œ ìƒì„±
                with st.spinner("ì‹œí—˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    exam_questions = generate_exam_questions(extracted_text,
                                                             lang)
                    st.session_state.exam_questions = exam_questions

                # GPTê°€ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸ ìƒì„±
                with st.spinner("GPTê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    gpt_questions = generate_questions_for_user(extracted_text,
                                                                lang)
                    st.session_state.gpt_questions = gpt_questions

                st.session_state.processed = True
        else:
            # ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ì„¸ì…˜ ìƒíƒœì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            extracted_text = st.session_state.extracted_text
            summary = st.session_state.summary
            key_summary_words = st.session_state.keywords
            term_info = st.session_state.term_info
            quiz = st.session_state.quiz
            exam_questions = st.session_state.exam_questions

        # ê²°ê³¼ í‘œì‹œ
        if 'summary' in st.session_state and st.session_state.summary.strip():
            st.write("## ìš”ì•½ ê²°ê³¼")
            st.write(st.session_state.summary)
        else:
            st.write("## ìš”ì•½ ê²°ê³¼ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if ('keywords' in st.session_state and
                st.session_state.keywords.strip()):
            st.write("## í•µì‹¬ ìš”ì•½ ë‹¨ì–´ ë° ì¶œì²˜")
            st.write(st.session_state.keywords)

        if ('term_info' in st.session_state and
                st.session_state.term_info.strip()):
            st.write("## ìš”ì•½ ë‚´ ì¤‘ìš”í•œ ë‹¨ì–´ ì •ë³´")
            st.write(st.session_state.term_info)

        if 'quiz' in st.session_state and st.session_state.quiz.strip():
            st.write("## ìƒì„±ëœ í€´ì¦ˆ")
            st.write(st.session_state.quiz)

        if ('exam_questions' in st.session_state and
                st.session_state.exam_questions.strip()):
            st.write("## ìƒì„±ëœ ì‹œí—˜ ë¬¸ì œ")
            st.write(st.session_state.exam_questions)

    elif extension in [".png", ".jpg", ".jpeg"]:
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ì½”ë“œ
        # íŒŒì¼ì˜ ê³ ìœ  í•´ì‹œ ìƒì„±
        file_bytes = uploaded_file.getvalue()
        file_hash = hashlib.md5(file_bytes).hexdigest()

        # íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if ("uploaded_file_hash" not in st.session_state or
                st.session_state.uploaded_file_hash != file_hash):
            st.session_state.uploaded_file_hash = file_hash

            # ì´ì „ì— ì €ì¥ëœ ê²°ê³¼ ì´ˆê¸°í™”
            st.session_state.extracted_text = ""
            st.session_state.summary = ""
            st.session_state.keywords = ""
            st.session_state.term_info = ""
            st.session_state.quiz = ""
            st.session_state.exam_questions = ""
            st.session_state.gpt_questions = []
            st.session_state.processed = False

        if not st.session_state.processed:
            extracted_text = image_to_text(uploaded_file)

            if not extracted_text.strip():
                st.error("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
                summary = ""
                st.session_state.summary = summary
            else:
                st.success("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
                language_code = detect_language(extracted_text)
                if language_code == 'ko':
                    lang = 'korean'
                    language_name = 'í•œêµ­ì–´'
                elif language_code == 'en':
                    lang = 'english'
                    language_name = 'ì˜ì–´'
                else:
                    lang = 'english'  # ê¸°ë³¸ê°’ì„ ì˜ì–´ë¡œ ì„¤ì •
                    language_name = 'ì•Œ ìˆ˜ ì—†ìŒ (ì˜ì–´ë¡œ ì§„í–‰í•©ë‹ˆë‹¤)'

                st.write(f"### ê°ì§€ëœ ì–¸ì–´: {language_name}")
                st.session_state.lang = lang
                st.session_state.extracted_text = extracted_text

                # ìš”ì•½ ìƒì„± ë° ì €ì¥
                with st.spinner("ìš”ì•½ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    summary = summarize_pdf(extracted_text, lang)
                    st.session_state.summary = summary

                # í•µì‹¬ ìš”ì•½ ë‹¨ì–´ ì¶”ì¶œ (ì¶œì²˜ í¬í•¨)
                with st.spinner("í•µì‹¬ ìš”ì•½ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    key_summary_words = extract_key_summary_words_with_sources(
                        extracted_text, lang)
                    st.session_state.keywords = key_summary_words

                # ì¤‘ìš” ë‹¨ì–´ ì •ë³´ ì¶”ì¶œ
                with st.spinner("ìš”ì•½ ë‚´ ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    term_info = extract_and_search_terms(summary,
                                                         extracted_text,
                                                         language=lang)
                    st.session_state.term_info = term_info

                # í€´ì¦ˆ ìƒì„±
                with st.spinner("í€´ì¦ˆë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    quiz = generate_quiz(extracted_text, lang)
                    st.session_state.quiz = quiz

                # ì‹œí—˜ ë¬¸ì œ ìƒì„±
                with st.spinner("ì‹œí—˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    exam_questions = generate_exam_questions(extracted_text,
                                                             lang)
                    st.session_state.exam_questions = exam_questions

                # GPTê°€ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸ ìƒì„±
                with st.spinner("GPTê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    gpt_questions = generate_questions_for_user(extracted_text,
                                                                lang)
                    st.session_state.gpt_questions = gpt_questions

                st.session_state.processed = True
        else:
            # ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ì„¸ì…˜ ìƒíƒœì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            extracted_text = st.session_state.extracted_text
            summary = st.session_state.summary
            key_summary_words = st.session_state.keywords
            term_info = st.session_state.term_info
            quiz = st.session_state.quiz
            exam_questions = st.session_state.exam_questions

        # ê²°ê³¼ í‘œì‹œ
        if 'summary' in st.session_state and st.session_state.summary.strip():
            st.write("## ìš”ì•½ ê²°ê³¼")
            st.write(st.session_state.summary)
        else:
            st.write("## ìš”ì•½ ê²°ê³¼ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if ('keywords' in st.session_state and
                st.session_state.keywords.strip()):
            st.write("## í•µì‹¬ ìš”ì•½ ë‹¨ì–´ ë° ì¶œì²˜")
            st.write(st.session_state.keywords)

        if ('term_info' in st.session_state and
                st.session_state.term_info.strip()):
            st.write("## ìš”ì•½ ë‚´ ì¤‘ìš”í•œ ë‹¨ì–´ ì •ë³´")
            st.write(st.session_state.term_info)

        if 'quiz' in st.session_state and st.session_state.quiz.strip():
            st.write("## ìƒì„±ëœ í€´ì¦ˆ")
            st.write(st.session_state.quiz)

        if ('exam_questions' in st.session_state and
                st.session_state.exam_questions.strip()):
            st.write("## ìƒì„±ëœ ì‹œí—˜ ë¬¸ì œ")
            st.write(st.session_state.exam_questions)

    else:
        st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF, PPTX ë˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì˜¬ë ¤ì£¼ì„¸ìš”.")

# í‚¤ì›Œë“œ ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€
if st.session_state.get("processed", False):
    st.write("---")
    if st.session_state.lang == 'korean':
        st.write("## ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰")
        search_query = st.text_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    else:
        st.write("## ğŸ” Keyword Search")
        search_query = st.text_input("Enter a keyword to search:")
    if search_query:
        # ê²€ìƒ‰ ê¸°ëŠ¥ êµ¬í˜„
        search_results = []
        for line in st.session_state.extracted_text.split('\n'):
            if search_query.lower() in line.lower():
                search_results.append(line.strip())
        if search_results:
            if st.session_state.lang == 'korean':
                st.write("### ê²€ìƒ‰ ê²°ê³¼:")
            else:
                st.write("### Search Results:")
            for result in search_results:
                st.write(f"- {result}")
        else:
            if st.session_state.lang == 'korean':
                st.write("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.write("No results found.")

# GPTê°€ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸í•˜ê³  ì‚¬ìš©ì ì‘ë‹µ ë°›ê¸°
if st.session_state.get("processed", False):
    st.write("---")
    if st.session_state.lang == 'korean':
        st.write("## GPTê°€ ë‹¹ì‹ ì—ê²Œ ì§ˆë¬¸í•©ë‹ˆë‹¤")
    else:
        st.write("## GPT has questions for you")
    if "gpt_questions" in st.session_state:
        for idx, question in enumerate(st.session_state.gpt_questions):
            user_answer = st.text_input(f"**{question}**", key=f"gpt_question_{idx}")
            if user_answer:
                with st.spinner("GPTê°€ ì‘ë‹µì„ ê²€í†  ì¤‘ì…ë‹ˆë‹¤..."):
                    if st.session_state.lang == 'korean':
                        feedback_prompt = f"{question}\n\nì‚¬ìš©ìì˜ ë‹µë³€: {user_answer}\n\nì´ì— ëŒ€í•´ í”¼ë“œë°±ì„ ì œê³µí•´ ì£¼ì„¸ìš”."
                    else:
                        feedback_prompt = f"{question}\n\nUser's answer: {user_answer}\n\nPlease provide feedback on this."
                    feedback = ask_gpt_question(feedback_prompt, st.session_state.lang)
                    if st.session_state.lang == 'korean':
                        st.write("### GPTì˜ í”¼ë“œë°±")
                    else:
                        st.write("### GPT's Feedback")
                    st.write(feedback)

# í•˜ë‹¨ì— ì£¼ì˜ ë¬¸êµ¬ ì¶”ê°€
st.write("---")
st.info("**ChatGPTëŠ” ì‹¤ìˆ˜ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.**") 
